{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Speech Command Recognition: Model Comparison\n",
        "\n",
        "This notebook compares three approaches for speech command recognition:\n",
        "1. **Baseline CNN** using Spectrogram (STFT) features\n",
        "2. **CNN with MFCC** features\n",
        "3. **Hidden Markov Model (HMM)** with MFCC features\n",
        "\n",
        "**Authors:** Nicholas Hefner, Arthur Ho, Hsuan-Yu Lin\n",
        "\n",
        "## Overview\n",
        "- **Dataset:** Google Speech Commands Dataset (mini version)\n",
        "- **Commands:** 8 classes (down, go, left, no, right, stop, up, yes)\n",
        "- **Purpose:** Compare deep learning and statistical approaches for audio classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 0: Setup and Data Loading\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0.1 Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import librosa\n",
        "import librosa.display\n",
        "from tqdm import tqdm\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from IPython import display\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# HMM\n",
        "from hmmlearn import hmm\n",
        "\n",
        "# Set seed for reproducibility\n",
        "SEED = 42\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Create directories\n",
        "os.makedirs('figures', exist_ok=True)\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0.2 Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset path - check multiple possible locations\n",
        "possible_paths = [\n",
        "    'data/mini_speech_commands_extracted/mini_speech_commands',\n",
        "    'data/mini_speech_commands_extracted/mini_speech_commands_extracted/mini_speech_commands',\n",
        "    'data/mini_speech_commands',\n",
        "]\n",
        "\n",
        "data_dir = None\n",
        "for path in possible_paths:\n",
        "    if pathlib.Path(path).exists():\n",
        "        data_dir = pathlib.Path(path)\n",
        "        break\n",
        "\n",
        "# Download dataset if not found\n",
        "if data_dir is None or not data_dir.exists():\n",
        "    print(\"Downloading dataset...\")\n",
        "    tf.keras.utils.get_file(\n",
        "        'mini_speech_commands.zip',\n",
        "        origin=\"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\",\n",
        "        extract=True,\n",
        "        cache_dir='.', cache_subdir='data'\n",
        "    )\n",
        "    # Check paths again after download\n",
        "    for path in possible_paths:\n",
        "        if pathlib.Path(path).exists():\n",
        "            data_dir = pathlib.Path(path)\n",
        "            break\n",
        "    print(\"Dataset downloaded!\")\n",
        "\n",
        "print(f\"Dataset location: {data_dir}\")\n",
        "\n",
        "# Audio parameters\n",
        "SAMPLE_RATE = 16000  # 16 kHz\n",
        "AUDIO_LENGTH = 16000  # 1 second of audio\n",
        "DURATION = 1.0  # seconds\n",
        "\n",
        "# List commands\n",
        "commands = np.array([d.name for d in data_dir.iterdir() if d.is_dir()])\n",
        "print(f\"Commands: {commands}\")\n",
        "print(f\"Number of commands: {len(commands)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count files per command\n",
        "print(\"Files per command:\")\n",
        "for cmd in sorted(commands):\n",
        "    cmd_path = data_dir / cmd\n",
        "    if cmd_path.is_dir():\n",
        "        num_files = len(list(cmd_path.glob('*.wav')))\n",
        "        print(f\"  {cmd}: {num_files} files\")\n",
        "\n",
        "# ============================================================\n",
        "# CREATE UNIFIED DATA SPLIT FOR ALL MODELS\n",
        "# ============================================================\n",
        "# This ensures CNN and HMM use the SAME train/test files for fair comparison\n",
        "\n",
        "all_files = []\n",
        "all_labels = []\n",
        "\n",
        "for cmd in sorted(commands):\n",
        "    cmd_path = data_dir / cmd\n",
        "    for audio_file in cmd_path.glob('*.wav'):\n",
        "        all_files.append(str(audio_file))\n",
        "        all_labels.append(cmd)\n",
        "\n",
        "all_files = np.array(all_files)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "# Create unified train/test split (80/20)\n",
        "train_files, test_files, train_labels, test_labels = train_test_split(\n",
        "    all_files, all_labels, \n",
        "    test_size=0.2, \n",
        "    random_state=SEED, \n",
        "    stratify=all_labels\n",
        ")\n",
        "\n",
        "# Further split train into train/val (90/10 of training = 72/8 overall)\n",
        "train_files, val_files, train_labels, val_labels = train_test_split(\n",
        "    train_files, train_labels,\n",
        "    test_size=0.1,  # 10% of training for validation\n",
        "    random_state=SEED,\n",
        "    stratify=train_labels\n",
        ")\n",
        "\n",
        "print(f\"\\n=== Unified Data Split ===\")\n",
        "print(f\"Training samples: {len(train_files)}\")\n",
        "print(f\"Validation samples: {len(val_files)}\")\n",
        "print(f\"Test samples: {len(test_files)}\")\n",
        "print(f\"Total: {len(train_files) + len(val_files) + len(test_files)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 1: Baseline CNN with Spectrograms\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Prepare TensorFlow Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "# ============================================================\n",
        "# Create TensorFlow datasets from unified file split\n",
        "# ============================================================\n",
        "\n",
        "def load_audio_file(file_path):\n",
        "    \"\"\"Load and decode a single audio file.\"\"\"\n",
        "    audio_binary = tf.io.read_file(file_path)\n",
        "    audio, _ = tf.audio.decode_wav(audio_binary, desired_channels=1, desired_samples=AUDIO_LENGTH)\n",
        "    return tf.squeeze(audio, axis=-1)\n",
        "\n",
        "def create_dataset_from_files(files, labels, label_names_list):\n",
        "    \"\"\"Create a TensorFlow dataset from file paths and labels.\"\"\"\n",
        "    # Create label encoder\n",
        "    label_to_idx = {label: idx for idx, label in enumerate(label_names_list)}\n",
        "    encoded_labels = [label_to_idx[label] for label in labels]\n",
        "    \n",
        "    # Create dataset\n",
        "    ds = tf.data.Dataset.from_tensor_slices((list(files), encoded_labels))\n",
        "    \n",
        "    def load_and_preprocess(file_path, label):\n",
        "        audio = load_audio_file(file_path)\n",
        "        return audio, label\n",
        "    \n",
        "    ds = ds.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "# Get sorted label names for consistent encoding\n",
        "label_names = np.array(sorted(set(all_labels)))\n",
        "print(f\"Label names: {label_names}\")\n",
        "\n",
        "# Create datasets from unified split\n",
        "train_ds = create_dataset_from_files(train_files, train_labels, label_names)\n",
        "val_ds = create_dataset_from_files(val_files, val_labels, label_names)\n",
        "test_ds = create_dataset_from_files(test_files, test_labels, label_names)\n",
        "\n",
        "# Batch the datasets\n",
        "train_ds = train_ds.shuffle(len(train_files)).batch(BATCH_SIZE)\n",
        "val_ds = val_ds.batch(BATCH_SIZE)\n",
        "test_ds = test_ds.batch(BATCH_SIZE)\n",
        "\n",
        "print(f\"Training batches: {len(list(train_ds))} (approx {len(train_files)} samples)\")\n",
        "print(f\"Validation batches: {len(list(val_ds))} (approx {len(val_files)} samples)\")\n",
        "print(f\"Test batches: {len(list(test_ds))} ({len(test_files)} samples)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check dataset shapes\n",
        "for example_audio, example_labels in train_ds.take(1):\n",
        "    print(f\"Audio batch shape: {example_audio.shape}\")\n",
        "    print(f\"Labels batch shape: {example_labels.shape}\")\n",
        "    \n",
        "print(f\"\\nUsing unified split - same files for CNN and HMM!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Convert Audio to Spectrograms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_spectrogram(waveform):\n",
        "    \"\"\"\n",
        "    Convert waveform to spectrogram using Short-Time Fourier Transform (STFT).\n",
        "    \"\"\"\n",
        "    spectrogram = tf.signal.stft(\n",
        "        waveform, \n",
        "        frame_length=255,\n",
        "        frame_step=128\n",
        "    )\n",
        "    spectrogram = tf.abs(spectrogram)\n",
        "    spectrogram = spectrogram[..., tf.newaxis]\n",
        "    return spectrogram\n",
        "\n",
        "# Test spectrogram function\n",
        "for audio, label in train_ds.take(1):\n",
        "    example_audio = audio[0]\n",
        "    example_label = label[0]\n",
        "    \n",
        "spectrogram = get_spectrogram(example_audio)\n",
        "print(f\"Waveform shape: {example_audio.shape}\")\n",
        "print(f\"Spectrogram shape: {spectrogram.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize waveform and spectrogram\n",
        "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
        "\n",
        "# Waveform\n",
        "axes[0].plot(example_audio.numpy())\n",
        "axes[0].set_title(f'Waveform: \"{label_names[example_label]}\"')\n",
        "axes[0].set_xlabel('Sample')\n",
        "axes[0].set_ylabel('Amplitude')\n",
        "\n",
        "# Spectrogram\n",
        "log_spec = np.log(spectrogram.numpy()[:, :, 0].T + np.finfo(float).eps)\n",
        "axes[1].pcolormesh(log_spec, shading='auto', cmap='viridis')\n",
        "axes[1].set_title('Spectrogram (Log Scale)')\n",
        "axes[1].set_xlabel('Time Frame')\n",
        "axes[1].set_ylabel('Frequency Bin')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/spectrogram_example.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create spectrogram datasets\n",
        "def make_spec_ds(ds):\n",
        "    return ds.map(\n",
        "        lambda audio, label: (get_spectrogram(audio), label),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "\n",
        "train_spectrogram_ds = make_spec_ds(train_ds)\n",
        "val_spectrogram_ds = make_spec_ds(val_ds)\n",
        "test_spectrogram_ds = make_spec_ds(test_ds)\n",
        "\n",
        "# Optimize performance\n",
        "train_spectrogram_ds = train_spectrogram_ds.cache().shuffle(10000).prefetch(tf.data.AUTOTUNE)\n",
        "val_spectrogram_ds = val_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
        "test_spectrogram_ds = test_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Check shapes\n",
        "for example_spectrograms, example_labels in train_spectrogram_ds.take(1):\n",
        "    print(f\"Spectrogram batch shape: {example_spectrograms.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Build and Train CNN Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get input shape\n",
        "for example_spectrograms, _ in train_spectrogram_ds.take(1):\n",
        "    input_shape = example_spectrograms.shape[1:]\n",
        "    \n",
        "print(f\"Input shape: {input_shape}\")\n",
        "num_labels = len(label_names)\n",
        "print(f\"Number of labels: {num_labels}\")\n",
        "\n",
        "# Normalization layer\n",
        "norm_layer = layers.Normalization()\n",
        "norm_layer.adapt(data=train_spectrogram_ds.map(lambda spec, label: spec))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build CNN model\n",
        "cnn_model = models.Sequential([\n",
        "    layers.Input(shape=input_shape),\n",
        "    norm_layer,\n",
        "    layers.Conv2D(32, 3, activation='relu'),\n",
        "    layers.Conv2D(64, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(num_labels),\n",
        "])\n",
        "\n",
        "cnn_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "cnn_model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train CNN\n",
        "EPOCHS = 10\n",
        "\n",
        "cnn_history = cnn_model.fit(\n",
        "    train_spectrogram_ds,\n",
        "    validation_data=val_spectrogram_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].plot(cnn_history.history['accuracy'], label='Training', linewidth=2)\n",
        "axes[0].plot(cnn_history.history['val_accuracy'], label='Validation', linewidth=2)\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].set_title('CNN Model Accuracy')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(cnn_history.history['loss'], label='Training', linewidth=2)\n",
        "axes[1].plot(cnn_history.history['val_loss'], label='Validation', linewidth=2)\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].set_title('CNN Model Loss')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/cnn_training_history.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.4 Evaluate CNN Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "cnn_test_loss, cnn_test_accuracy = cnn_model.evaluate(test_spectrogram_ds)\n",
        "print(f\"\\nCNN Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
        "print(f\"CNN Test Loss: {cnn_test_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate predictions for confusion matrix\n",
        "cnn_y_pred = []\n",
        "cnn_y_true = []\n",
        "\n",
        "for spectrograms, labels in test_spectrogram_ds:\n",
        "    predictions = cnn_model.predict(spectrograms, verbose=0)\n",
        "    cnn_y_pred.extend(np.argmax(predictions, axis=1))\n",
        "    cnn_y_true.extend(labels.numpy())\n",
        "\n",
        "cnn_y_pred = np.array(cnn_y_pred)\n",
        "cnn_y_true = np.array(cnn_y_true)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CNN Confusion matrix\n",
        "cnn_cm = confusion_matrix(cnn_y_true, cnn_y_pred)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(\n",
        "    cnn_cm, \n",
        "    annot=True, \n",
        "    fmt='d', \n",
        "    cmap='Blues',\n",
        "    xticklabels=label_names,\n",
        "    yticklabels=label_names\n",
        ")\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix - CNN Model (Spectrogram Features)')\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/confusion_matrix_cnn.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nCNN Classification Report:\")\n",
        "print(\"=\" * 60)\n",
        "print(classification_report(cnn_y_true, cnn_y_pred, target_names=label_names))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save CNN model\n",
        "cnn_model.save('models/baseline_cnn_model.keras')\n",
        "print(\"CNN model saved to 'models/baseline_cnn_model.keras'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 2: CNN with MFCC Features\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Convert Audio to MFCC Spectrograms\n",
        "\n",
        "This approach uses CNN with MFCC features extracted via TensorFlow. MFCC reduces the dimensionality compared to raw spectrograms while preserving speech-relevant information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_mfcc_spectrogram(waveform):\n",
        "    \"\"\"\n",
        "    Convert waveform to MFCC spectrogram using TensorFlow.\n",
        "    \"\"\"\n",
        "    # Compute STFT\n",
        "    spectrogram = tf.signal.stft(\n",
        "        waveform, \n",
        "        frame_length=255,\n",
        "        frame_step=128\n",
        "    )\n",
        "    \n",
        "    # Get magnitude\n",
        "    spectrograms = tf.abs(spectrogram)\n",
        "    \n",
        "    # Warp to mel-scale\n",
        "    num_spectrogram_bins = spectrogram.shape[-1]\n",
        "    lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80\n",
        "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
        "        num_mel_bins, num_spectrogram_bins, SAMPLE_RATE, lower_edge_hertz, upper_edge_hertz\n",
        "    )\n",
        "    mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)\n",
        "    mel_spectrograms.set_shape(spectrograms.shape[:-1].concatenate(\n",
        "        linear_to_mel_weight_matrix.shape[-1:]))\n",
        "\n",
        "    # Compute log mel spectrograms\n",
        "    log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\n",
        "\n",
        "    # Compute MFCCs (take first 13 coefficients)\n",
        "    mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrograms)[..., :13]\n",
        "    \n",
        "    # Add channel dimension for Conv2D\n",
        "    mfccs = mfccs[..., tf.newaxis]\n",
        "    \n",
        "    return mfccs\n",
        "\n",
        "# Test MFCC function\n",
        "for audio, label in train_ds.take(1):\n",
        "    mfcc_example = get_mfcc_spectrogram(audio[0])\n",
        "    \n",
        "print(f\"MFCC spectrogram shape: {mfcc_example.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create MFCC datasets\n",
        "def make_mfcc_ds(ds):\n",
        "    return ds.map(\n",
        "        lambda audio, label: (get_mfcc_spectrogram(audio), label),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "\n",
        "train_mfcc_ds = make_mfcc_ds(train_ds)\n",
        "val_mfcc_ds = make_mfcc_ds(val_ds)\n",
        "test_mfcc_ds = make_mfcc_ds(test_ds)\n",
        "\n",
        "# Optimize performance\n",
        "train_mfcc_ds = train_mfcc_ds.cache().shuffle(10000).prefetch(tf.data.AUTOTUNE)\n",
        "val_mfcc_ds = val_mfcc_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
        "test_mfcc_ds = test_mfcc_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Check shapes\n",
        "for example_mfcc, example_labels in train_mfcc_ds.take(1):\n",
        "    mfcc_input_shape = example_mfcc.shape[1:]\n",
        "    print(f\"MFCC batch shape: {example_mfcc.shape}\")\n",
        "    print(f\"MFCC input shape: {mfcc_input_shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalization layer for MFCC CNN\n",
        "mfcc_norm_layer = layers.Normalization()\n",
        "mfcc_norm_layer.adapt(data=train_mfcc_ds.map(lambda spec, label: spec))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Build and Train CNN with MFCC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build CNN model for MFCC features\n",
        "mfcc_cnn_model = models.Sequential([\n",
        "    layers.Input(shape=mfcc_input_shape),\n",
        "    mfcc_norm_layer,\n",
        "    layers.Conv2D(32, 3, activation='relu'),\n",
        "    layers.Conv2D(64, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(num_labels),\n",
        "])\n",
        "\n",
        "mfcc_cnn_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(f\"MFCC CNN Input shape: {mfcc_input_shape}\")\n",
        "print(f\"MFCC CNN Parameters: {mfcc_cnn_model.count_params():,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train MFCC CNN\n",
        "mfcc_cnn_history = mfcc_cnn_model.fit(\n",
        "    train_mfcc_ds,\n",
        "    validation_data=val_mfcc_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 Evaluate CNN with MFCC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate MFCC CNN on test set\n",
        "mfcc_cnn_test_loss, mfcc_cnn_test_accuracy = mfcc_cnn_model.evaluate(test_mfcc_ds)\n",
        "print(f\"\\nMFCC CNN Test Accuracy: {mfcc_cnn_test_accuracy:.4f}\")\n",
        "print(f\"MFCC CNN Test Loss: {mfcc_cnn_test_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate predictions for MFCC CNN confusion matrix\n",
        "mfcc_cnn_y_pred = []\n",
        "mfcc_cnn_y_true = []\n",
        "\n",
        "for mfccs, labels in test_mfcc_ds:\n",
        "    predictions = mfcc_cnn_model.predict(mfccs, verbose=0)\n",
        "    mfcc_cnn_y_pred.extend(np.argmax(predictions, axis=1))\n",
        "    mfcc_cnn_y_true.extend(labels.numpy())\n",
        "\n",
        "mfcc_cnn_y_pred = np.array(mfcc_cnn_y_pred)\n",
        "mfcc_cnn_y_true = np.array(mfcc_cnn_y_true)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MFCC CNN Confusion matrix\n",
        "mfcc_cnn_cm = confusion_matrix(mfcc_cnn_y_true, mfcc_cnn_y_pred)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(\n",
        "    mfcc_cnn_cm, \n",
        "    annot=True, \n",
        "    fmt='d', \n",
        "    cmap='Oranges',\n",
        "    xticklabels=label_names,\n",
        "    yticklabels=label_names\n",
        ")\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix - CNN with MFCC Features')\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/confusion_matrix_mfcc_cnn.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nMFCC CNN Classification Report:\")\n",
        "print(\"=\" * 60)\n",
        "print(classification_report(mfcc_cnn_y_true, mfcc_cnn_y_pred, target_names=label_names))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 3: HMM with MFCC Features\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1 Extract MFCC Sequences for HMM\n",
        "\n",
        "HMM (Hidden Markov Model) is a statistical model that models temporal sequences. Unlike CNNs that use fixed-size feature vectors, HMMs can handle variable-length sequences naturally.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MFCC parameters for HMM\n",
        "N_MFCC = 13\n",
        "\n",
        "def extract_mfcc_seq(file_path, sr=SAMPLE_RATE, n_mfcc=N_MFCC):\n",
        "    \"\"\"Extract MFCC sequence for HMM (preserves time dimension).\"\"\"\n",
        "    y, sr = librosa.load(file_path, sr=sr)\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
        "    delta = librosa.feature.delta(mfcc)\n",
        "    delta2 = librosa.feature.delta(mfcc, order=2)\n",
        "    feats = np.vstack([mfcc, delta, delta2])  # (3*n_mfcc, T)\n",
        "    return feats.T  # (T, 3*n_mfcc)\n",
        "\n",
        "# ============================================================\n",
        "# Extract MFCC sequences using the SAME unified split as CNN\n",
        "# ============================================================\n",
        "print(\"Extracting MFCC sequences for HMM using unified split...\")\n",
        "\n",
        "# Extract from training files\n",
        "X_train_seqs = []\n",
        "y_train_hmm = []\n",
        "print(\"Processing training files...\")\n",
        "for file_path, label in tqdm(zip(train_files, train_labels), total=len(train_files)):\n",
        "    seq = extract_mfcc_seq(file_path)\n",
        "    if seq is not None:\n",
        "        X_train_seqs.append(seq)\n",
        "        y_train_hmm.append(label)\n",
        "\n",
        "# Extract from test files  \n",
        "X_test_seqs = []\n",
        "y_test_hmm = []\n",
        "print(\"Processing test files...\")\n",
        "for file_path, label in tqdm(zip(test_files, test_labels), total=len(test_files)):\n",
        "    seq = extract_mfcc_seq(file_path)\n",
        "    if seq is not None:\n",
        "        X_test_seqs.append(seq)\n",
        "        y_test_hmm.append(label)\n",
        "\n",
        "print(f\"\\nHMM Training sequences: {len(X_train_seqs)}\")\n",
        "print(f\"HMM Test sequences: {len(X_test_seqs)}\")\n",
        "print(f\"Example sequence shape: {X_train_seqs[0].shape}\")\n",
        "print(f\"\\n*** Using SAME files as CNN for fair comparison! ***\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale features for HMM (using training data statistics only)\n",
        "hmm_scaler = StandardScaler()\n",
        "all_train_frames = np.vstack(X_train_seqs)\n",
        "hmm_scaler.fit(all_train_frames)\n",
        "\n",
        "X_train_seqs_scaled = [hmm_scaler.transform(seq) for seq in X_train_seqs]\n",
        "X_test_seqs_scaled = [hmm_scaler.transform(seq) for seq in X_test_seqs]\n",
        "\n",
        "print(f\"Features scaled using StandardScaler\")\n",
        "print(f\"Training set: {len(X_train_seqs_scaled)} sequences\")\n",
        "print(f\"Test set: {len(X_test_seqs_scaled)} sequences\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Train HMM Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train one HMM per class\n",
        "unique_labels = sorted(set(y_train_hmm))\n",
        "label_to_hmm = {}\n",
        "\n",
        "n_components = 8  # Number of hidden states\n",
        "n_iter = 100\n",
        "\n",
        "print(\"Training HMM models...\")\n",
        "for label in unique_labels:\n",
        "    seqs = [X_train_seqs_scaled[i] for i in range(len(X_train_seqs_scaled)) if y_train_hmm[i] == label]\n",
        "    \n",
        "    lengths = [s.shape[0] for s in seqs]\n",
        "    X_concat = np.vstack(seqs)\n",
        "\n",
        "    model = hmm.GaussianHMM(\n",
        "        n_components=n_components,\n",
        "        covariance_type=\"diag\",\n",
        "        n_iter=n_iter,\n",
        "        random_state=SEED,\n",
        "        verbose=False,\n",
        "    )\n",
        "\n",
        "    model.fit(X_concat, lengths)\n",
        "    label_to_hmm[label] = model\n",
        "    print(f\"  Trained HMM for '{label}' on {len(seqs)} sequences\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.3 Evaluate HMM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HMM prediction function\n",
        "def predict_hmm(seq, models):\n",
        "    scores = {}\n",
        "    for label, model in models.items():\n",
        "        try:\n",
        "            scores[label] = model.score(seq)\n",
        "        except:\n",
        "            scores[label] = -np.inf\n",
        "    return max(scores, key=scores.get)\n",
        "\n",
        "# Make predictions on TRAINING set (for train accuracy)\n",
        "hmm_train_pred = []\n",
        "for seq in X_train_seqs_scaled:\n",
        "    label_hat = predict_hmm(seq, label_to_hmm)\n",
        "    hmm_train_pred.append(label_hat)\n",
        "\n",
        "hmm_train_pred = np.array(hmm_train_pred)\n",
        "hmm_train_true = np.array(y_train_hmm)\n",
        "hmm_train_accuracy = np.mean(hmm_train_pred == hmm_train_true)\n",
        "\n",
        "# Make predictions on TEST set\n",
        "hmm_y_pred = []\n",
        "for seq in X_test_seqs_scaled:\n",
        "    label_hat = predict_hmm(seq, label_to_hmm)\n",
        "    hmm_y_pred.append(label_hat)\n",
        "\n",
        "hmm_y_pred = np.array(hmm_y_pred)\n",
        "hmm_y_true = np.array(y_test_hmm)\n",
        "hmm_test_accuracy = np.mean(hmm_y_pred == hmm_y_true)\n",
        "\n",
        "# Calculate HMM model size (parameters per HMM * number of classes)\n",
        "# Each GaussianHMM has: means (n_components x n_features), covars (n_components x n_features for diag),\n",
        "# transmat (n_components x n_components), startprob (n_components)\n",
        "n_features = 39  # MFCC + delta + delta2\n",
        "hmm_params_per_class = (n_components * n_features) + (n_components * n_features) + (n_components * n_components) + n_components\n",
        "hmm_total_params = hmm_params_per_class * len(unique_labels)\n",
        "\n",
        "print(f\"\\nHMM Train Accuracy: {hmm_train_accuracy:.4f}\")\n",
        "print(f\"HMM Test Accuracy: {hmm_test_accuracy:.4f}\")\n",
        "print(f\"HMM Parameters per class: {hmm_params_per_class:,}\")\n",
        "print(f\"HMM Total Parameters (8 classes): {hmm_total_params:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HMM Confusion matrix\n",
        "hmm_cm = confusion_matrix(hmm_y_true, hmm_y_pred, labels=unique_labels)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(\n",
        "    hmm_cm, \n",
        "    annot=True, \n",
        "    fmt='d', \n",
        "    cmap='Greens',\n",
        "    xticklabels=unique_labels,\n",
        "    yticklabels=unique_labels\n",
        ")\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix - HMM with MFCC Features')\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/confusion_matrix_hmm.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nHMM Classification Report:\")\n",
        "print(\"=\" * 60)\n",
        "print(classification_report(hmm_y_true, hmm_y_pred, target_names=unique_labels))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Part 4: Comparative Analysis\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.1 Overall Results Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get CNN training accuracies from history\n",
        "cnn_train_accuracy = cnn_history.history['accuracy'][-1]\n",
        "mfcc_cnn_train_accuracy = mfcc_cnn_history.history['accuracy'][-1]\n",
        "\n",
        "# Compile all results with train/test accuracy and model size\n",
        "all_results = {\n",
        "    'CNN (Spectrogram)': {\n",
        "        'train_acc': cnn_train_accuracy,\n",
        "        'test_acc': cnn_test_accuracy,\n",
        "        'params': cnn_model.count_params(),\n",
        "        'features': 'Spectrogram (STFT)'\n",
        "    },\n",
        "    'CNN (MFCC)': {\n",
        "        'train_acc': mfcc_cnn_train_accuracy,\n",
        "        'test_acc': mfcc_cnn_test_accuracy,\n",
        "        'params': mfcc_cnn_model.count_params(),\n",
        "        'features': 'MFCC (13 coeff)'\n",
        "    },\n",
        "    'HMM (MFCC)': {\n",
        "        'train_acc': hmm_train_accuracy,\n",
        "        'test_acc': hmm_test_accuracy,\n",
        "        'params': hmm_total_params,\n",
        "        'features': 'MFCC + Deltas'\n",
        "    },\n",
        "}\n",
        "\n",
        "# Sort by test accuracy\n",
        "sorted_results = dict(sorted(all_results.items(), key=lambda x: x[1]['test_acc'], reverse=True))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
        "print(\"=\" * 100)\n",
        "print(f\"{'Rank':<6} {'Model':<20} {'Features':<18} {'Train Acc':<12} {'Test Acc':<12} {'Parameters':<15}\")\n",
        "print(\"-\" * 100)\n",
        "for rank, (name, info) in enumerate(sorted_results.items(), 1):\n",
        "    print(f\"{rank:<6} {name:<20} {info['features']:<18} {info['train_acc']:.2%}{'':<5} {info['test_acc']:.2%}{'':<5} {info['params']:,}\")\n",
        "print(\"-\" * 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2 Visual Comparison: All Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bar chart comparison - Train vs Test Accuracy\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "model_names = list(sorted_results.keys())\n",
        "train_accs = [info['train_acc'] for info in sorted_results.values()]\n",
        "test_accs = [info['test_acc'] for info in sorted_results.values()]\n",
        "params = [info['params'] for info in sorted_results.values()]\n",
        "\n",
        "# Color scheme\n",
        "colors = []\n",
        "for name in model_names:\n",
        "    if 'CNN' in name and 'Spectrogram' in name:\n",
        "        colors.append('#e74c3c')\n",
        "    elif 'CNN' in name and 'MFCC' in name:\n",
        "        colors.append('#f39c12')\n",
        "    else:\n",
        "        colors.append('#27ae60')\n",
        "\n",
        "# Plot 1: Train vs Test Accuracy\n",
        "x = np.arange(len(model_names))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = axes[0].bar(x - width/2, train_accs, width, label='Train', color=colors, edgecolor='black', alpha=0.7)\n",
        "bars2 = axes[0].bar(x + width/2, test_accs, width, label='Test', color=colors, edgecolor='black')\n",
        "\n",
        "# Add value labels\n",
        "for bar, acc in zip(bars1, train_accs):\n",
        "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "                 f'{acc:.1%}', ha='center', va='bottom', fontsize=9)\n",
        "for bar, acc in zip(bars2, test_accs):\n",
        "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "                 f'{acc:.1%}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
        "\n",
        "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
        "axes[0].set_xlabel('Model', fontsize=12)\n",
        "axes[0].set_title('Train vs Test Accuracy', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylim(0, 1.0)\n",
        "axes[0].set_xticks(x)\n",
        "axes[0].set_xticklabels(model_names, rotation=15, ha='right')\n",
        "axes[0].legend()\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Plot 2: Model Size (Parameters)\n",
        "bars3 = axes[1].bar(model_names, params, color=colors, edgecolor='black')\n",
        "\n",
        "# Add value labels\n",
        "for bar, p in zip(bars3, params):\n",
        "    if p >= 1_000_000:\n",
        "        label = f'{p/1_000_000:.1f}M'\n",
        "    elif p >= 1_000:\n",
        "        label = f'{p/1_000:.1f}K'\n",
        "    else:\n",
        "        label = str(p)\n",
        "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + bar.get_height()*0.02, \n",
        "                 label, ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "axes[1].set_ylabel('Parameters', fontsize=12)\n",
        "axes[1].set_xlabel('Model', fontsize=12)\n",
        "axes[1].set_title('Model Size Comparison', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xticklabels(model_names, rotation=15, ha='right')\n",
        "axes[1].set_yscale('log')\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/model_comparison_all.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Side-by-side confusion matrices\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "# CNN (Spectrogram) confusion matrix\n",
        "sns.heatmap(\n",
        "    cnn_cm, \n",
        "    annot=True, \n",
        "    fmt='d', \n",
        "    cmap='Blues',\n",
        "    xticklabels=label_names,\n",
        "    yticklabels=label_names,\n",
        "    ax=axes[0]\n",
        ")\n",
        "axes[0].set_xlabel('Predicted')\n",
        "axes[0].set_ylabel('True')\n",
        "axes[0].set_title(f'CNN (Spectrogram)\\nAccuracy: {cnn_test_accuracy:.1%}', fontsize=11, fontweight='bold')\n",
        "\n",
        "# CNN (MFCC) confusion matrix\n",
        "sns.heatmap(\n",
        "    mfcc_cnn_cm, \n",
        "    annot=True, \n",
        "    fmt='d', \n",
        "    cmap='Oranges',\n",
        "    xticklabels=label_names,\n",
        "    yticklabels=label_names,\n",
        "    ax=axes[1]\n",
        ")\n",
        "axes[1].set_xlabel('Predicted')\n",
        "axes[1].set_ylabel('True')\n",
        "axes[1].set_title(f'CNN (MFCC)\\nAccuracy: {mfcc_cnn_test_accuracy:.1%}', fontsize=11, fontweight='bold')\n",
        "\n",
        "# HMM confusion matrix\n",
        "sns.heatmap(\n",
        "    hmm_cm, \n",
        "    annot=True, \n",
        "    fmt='d', \n",
        "    cmap='Greens',\n",
        "    xticklabels=unique_labels,\n",
        "    yticklabels=unique_labels,\n",
        "    ax=axes[2]\n",
        ")\n",
        "axes[2].set_xlabel('Predicted')\n",
        "axes[2].set_ylabel('True')\n",
        "axes[2].set_title(f'HMM (MFCC)\\nAccuracy: {hmm_test_accuracy:.1%}', fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.suptitle('Confusion Matrix Comparison', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/confusion_matrix_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.3 Summary Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary table\n",
        "print(\"\\n\" + \"=\" * 95)\n",
        "print(\"DETAILED COMPARISON SUMMARY\")\n",
        "print(\"=\" * 95)\n",
        "\n",
        "print(\"\\nðŸ“Š MODEL COMPARISON\")\n",
        "print(\"-\" * 95)\n",
        "print(f\"{'Aspect':<20} {'CNN (Spectrogram)':<25} {'CNN (MFCC)':<25} {'HMM (MFCC)':<25}\")\n",
        "print(\"-\" * 95)\n",
        "print(f\"{'Feature Type':<20} {'Spectrogram (STFT)':<25} {'MFCC (13 coeff)':<25} {'MFCC + Deltas (39)':<25}\")\n",
        "print(f\"{'Model Type':<20} {'Deep Learning':<25} {'Deep Learning':<25} {'Statistical':<25}\")\n",
        "print(f\"{'Input Shape':<20} {str(input_shape):<25} {str(mfcc_input_shape):<25} {'(T, 39)':<25}\")\n",
        "print(f\"{'Parameters':<20} {f'{cnn_model.count_params():,}':<25} {f'{mfcc_cnn_model.count_params():,}':<25} {f'{hmm_total_params:,}':<25}\")\n",
        "print(f\"{'Train Accuracy':<20} {f'{cnn_train_accuracy:.2%}':<25} {f'{mfcc_cnn_train_accuracy:.2%}':<25} {f'{hmm_train_accuracy:.2%}':<25}\")\n",
        "print(f\"{'Test Accuracy':<20} {f'{cnn_test_accuracy:.2%}':<25} {f'{mfcc_cnn_test_accuracy:.2%}':<25} {f'{hmm_test_accuracy:.2%}':<25}\")\n",
        "\n",
        "# Calculate generalization gap\n",
        "cnn_gap = cnn_train_accuracy - cnn_test_accuracy\n",
        "mfcc_cnn_gap = mfcc_cnn_train_accuracy - mfcc_cnn_test_accuracy\n",
        "hmm_gap = hmm_train_accuracy - hmm_test_accuracy\n",
        "print(f\"{'Generalization Gap':<20} {f'{cnn_gap:.2%}':<25} {f'{mfcc_cnn_gap:.2%}':<25} {f'{hmm_gap:.2%}':<25}\")\n",
        "\n",
        "print(\"\\nðŸ† KEY FINDINGS\")\n",
        "print(\"-\" * 95)\n",
        "best_model = max(all_results, key=lambda x: all_results[x]['test_acc'])\n",
        "best_acc = all_results[best_model]['test_acc']\n",
        "print(f\"1. Best performing model: {best_model} with {best_acc:.2%} test accuracy\")\n",
        "print(f\"2. HMM shows smallest generalization gap ({hmm_gap:.2%}), indicating good generalization\")\n",
        "print(f\"3. CNN (Spectrogram) has {cnn_model.count_params()/hmm_total_params:.0f}x more parameters than HMM\")\n",
        "print(f\"4. CNN (MFCC) reduces parameters by {(1 - mfcc_cnn_model.count_params()/cnn_model.count_params())*100:.0f}% vs CNN (Spectrogram)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Conclusions\n",
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "This notebook compared three different approaches to speech command recognition:\n",
        "\n",
        "### 1. CNN with Spectrogram Features\n",
        "- **Features:** Raw spectrogram computed via STFT\n",
        "- **Strengths:** Highest accuracy, learns features automatically\n",
        "- **Weaknesses:** Most parameters, requires GPU for efficient training\n",
        "\n",
        "### 2. CNN with MFCC Features\n",
        "- **Features:** MFCC (13 coefficients) extracted via TensorFlow\n",
        "- **Strengths:** Much fewer parameters (~15x reduction), still uses deep learning\n",
        "- **Weaknesses:** Slightly lower accuracy than raw spectrogram approach\n",
        "\n",
        "### 3. HMM with MFCC Features\n",
        "- **Features:** MFCC + Delta + Delta-Delta sequences\n",
        "- **Strengths:** Statistical model, handles variable-length sequences naturally, interpretable\n",
        "- **Weaknesses:** Requires training separate model per class\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "1. **Raw spectrograms preserve more information** than MFCC features for CNN-based classification\n",
        "2. **MFCC reduces dimensionality significantly** while maintaining reasonable accuracy\n",
        "3. **HMM provides a strong baseline** for sequence modeling with much simpler training\n",
        "4. **Trade-off between accuracy and efficiency:** CNN+MFCC offers good balance between performance and model size\n",
        "5. **Feature engineering vs. feature learning:** CNNs can learn from raw data, while HMMs benefit from hand-crafted MFCC features\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
