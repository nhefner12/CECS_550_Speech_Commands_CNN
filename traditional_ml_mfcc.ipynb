{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Traditional ML Models with MFCC Features\n",
        "\n",
        "This notebook implements traditional machine learning models using MFCC (Mel-Frequency Cepstral Coefficients) features for speech command recognition.\n",
        "\n",
        "**Authors:** Nicholas Hefner, Arthur Ho, Hsuan-Yu Lin\n",
        "\n",
        "## Overview\n",
        "- Dataset: Google Speech Commands Dataset  \n",
        "- Features: MFCC extracted using librosa\n",
        "- Models: SVM, Random Forest, KNN, etc.\n",
        "- Purpose: Compare traditional ML approaches with CNN baseline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import librosa\n",
        "import librosa.display\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Set seed for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset path (same as baseline CNN notebook)\n",
        "DATASET_PATH = 'data/mini_speech_commands_extracted/mini_speech_commands'\n",
        "data_dir = pathlib.Path(DATASET_PATH)\n",
        "\n",
        "# Audio parameters\n",
        "SAMPLE_RATE = 16000\n",
        "DURATION = 1.0  # seconds\n",
        "\n",
        "# List commands\n",
        "commands = [d.name for d in data_dir.iterdir() if d.is_dir()]\n",
        "print(f\"Commands: {commands}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Extract MFCC Features\n",
        "\n",
        "MFCC (Mel-Frequency Cepstral Coefficients) are commonly used features for speech recognition. They capture the power spectrum of audio in a way that mimics human auditory perception.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MFCC parameters\n",
        "N_MFCC = 13  # Number of MFCC coefficients\n",
        "N_FFT = 2048  # FFT window size\n",
        "HOP_LENGTH = 512  # Hop length for STFT\n",
        "\n",
        "def extract_mfcc(file_path, n_mfcc=N_MFCC, n_fft=N_FFT, hop_length=HOP_LENGTH):\n",
        "    \"\"\"\n",
        "    Extract MFCC features from an audio file.\n",
        "    \n",
        "    Parameters:\n",
        "    - file_path: Path to the audio file\n",
        "    - n_mfcc: Number of MFCC coefficients\n",
        "    - n_fft: FFT window size\n",
        "    - hop_length: Hop length for STFT\n",
        "    \n",
        "    Returns:\n",
        "    - mfcc_features: Mean MFCC features across time frames\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load audio file\n",
        "        audio, sr = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n",
        "        \n",
        "        # Pad or trim to ensure consistent length\n",
        "        target_length = int(SAMPLE_RATE * DURATION)\n",
        "        if len(audio) < target_length:\n",
        "            audio = np.pad(audio, (0, target_length - len(audio)))\n",
        "        else:\n",
        "            audio = audio[:target_length]\n",
        "        \n",
        "        # Extract MFCC\n",
        "        mfcc = librosa.feature.mfcc(\n",
        "            y=audio, \n",
        "            sr=sr, \n",
        "            n_mfcc=n_mfcc,\n",
        "            n_fft=n_fft,\n",
        "            hop_length=hop_length\n",
        "        )\n",
        "        \n",
        "        # Take mean across time frames (flatten to 1D)\n",
        "        mfcc_mean = np.mean(mfcc, axis=1)\n",
        "        \n",
        "        # Also include delta (first derivative) and delta-delta (second derivative)\n",
        "        mfcc_delta = librosa.feature.delta(mfcc)\n",
        "        mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n",
        "        \n",
        "        # Combine all features\n",
        "        features = np.concatenate([\n",
        "            np.mean(mfcc, axis=1),\n",
        "            np.mean(mfcc_delta, axis=1),\n",
        "            np.mean(mfcc_delta2, axis=1)\n",
        "        ])\n",
        "        \n",
        "        return features\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize MFCC for a sample audio file\n",
        "sample_file = list((data_dir / 'yes').glob('*.wav'))[0]\n",
        "audio, sr = librosa.load(sample_file, sr=SAMPLE_RATE)\n",
        "mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=N_MFCC)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "librosa.display.specshow(mfcc, x_axis='time', sr=sr, hop_length=HOP_LENGTH)\n",
        "plt.colorbar(format='%+2.0f dB')\n",
        "plt.title(f'MFCC - Sample \"yes\" audio')\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/mfcc_example.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"MFCC shape: {mfcc.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract features from all audio files\n",
        "X = []  # Features\n",
        "y = []  # Labels\n",
        "\n",
        "print(\"Extracting MFCC features from all audio files...\")\n",
        "for command in tqdm(commands):\n",
        "    command_dir = data_dir / command\n",
        "    for audio_file in command_dir.glob('*.wav'):\n",
        "        features = extract_mfcc(str(audio_file))\n",
        "        if features is not None:\n",
        "            X.append(features)\n",
        "            y.append(command)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
        "print(f\"Labels shape: {y.shape}\")\n",
        "print(f\"Feature vector length: {X.shape[1]} (13 MFCC + 13 delta + 13 delta-delta)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Prepare Data for Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "print(f\"Classes: {label_encoder.classes_}\")\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=SEED, stratify=y_encoded\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Features scaled using StandardScaler\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train Traditional ML Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dictionary to store results\n",
        "results = {}\n",
        "\n",
        "# Define models to train\n",
        "models = {\n",
        "    'SVM (RBF)': SVC(kernel='rbf', random_state=SEED),\n",
        "    'SVM (Linear)': SVC(kernel='linear', random_state=SEED),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=SEED),\n",
        "    'KNN (k=5)': KNeighborsClassifier(n_neighbors=5),\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and evaluate each model\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    \n",
        "    # Predictions\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    \n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    results[name] = accuracy\n",
        "    \n",
        "    print(f\"{name} Test Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluate Best Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find best model\n",
        "best_model_name = max(results, key=results.get)\n",
        "best_model = models[best_model_name]\n",
        "print(f\"Best model: {best_model_name} with accuracy: {results[best_model_name]:.4f}\")\n",
        "\n",
        "# Get predictions from best model\n",
        "y_pred_best = best_model.predict(X_test_scaled)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrix for best model\n",
        "cm = confusion_matrix(y_test, y_pred_best)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(\n",
        "    cm, \n",
        "    annot=True, \n",
        "    fmt='d', \n",
        "    cmap='Greens',\n",
        "    xticklabels=label_encoder.classes_,\n",
        "    yticklabels=label_encoder.classes_\n",
        ")\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title(f'Confusion Matrix - {best_model_name} with MFCC Features')\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/confusion_matrix_mfcc.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classification report\n",
        "print(f\"\\nClassification Report - {best_model_name}:\")\n",
        "print(\"=\" * 60)\n",
        "print(classification_report(y_test, y_pred_best, target_names=label_encoder.classes_))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Compare All Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot comparison of all models\n",
        "plt.figure(figsize=(10, 6))\n",
        "model_names = list(results.keys())\n",
        "accuracies = list(results.values())\n",
        "\n",
        "bars = plt.bar(model_names, accuracies, color=['#3498db', '#2ecc71', '#e74c3c', '#9b59b6'])\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.title('Model Comparison - MFCC Features')\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "# Add accuracy labels on bars\n",
        "for bar, acc in zip(bars, accuracies):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "             f'{acc:.3f}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/model_comparison_mfcc.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary table\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"TRADITIONAL ML MODELS RESULTS (MFCC Features)\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"{'Model':<20} {'Test Accuracy':<15}\")\n",
        "print(\"-\" * 35)\n",
        "for name, acc in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"{name:<20} {acc:.4f}\")\n",
        "print(\"-\" * 35)\n",
        "print(f\"\\nBest Model: {best_model_name}\")\n",
        "print(f\"Feature Type: MFCC (13 coefficients + delta + delta-delta = 39 features)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary and Comparison with Baseline CNN\n",
        "\n",
        "### Results Summary\n",
        "This notebook evaluated traditional ML models using MFCC features:\n",
        "- **Feature Extraction**: 13 MFCCs + 13 delta + 13 delta-delta = 39 features per audio sample\n",
        "- **Models Tested**: SVM (RBF & Linear), Random Forest, KNN\n",
        "\n",
        "### Comparison with Baseline CNN\n",
        "| Approach | Features | Model | Accuracy |\n",
        "|----------|----------|-------|----------|\n",
        "| Baseline CNN | Spectrogram (STFT) | CNN | See baseline_cnn_model.ipynb |\n",
        "| Traditional ML | MFCC | Best model above | See results above |\n",
        "\n",
        "### Key Differences\n",
        "1. **Feature Engineering**: CNN learns features automatically from spectrograms; traditional ML requires hand-crafted MFCC features\n",
        "2. **Model Complexity**: CNN has more parameters; traditional ML models are simpler\n",
        "3. **Training Time**: Traditional ML typically trains faster\n",
        "4. **Interpretability**: MFCC features are more interpretable than learned CNN features\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
